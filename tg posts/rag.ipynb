{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install telethon sentence-transformers faiss-cpu langchain sqlite3 numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbiRrjZd4gEa",
        "outputId": "54f7077a-e185-4d91-d0a8-9130db593931"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting telethon\n",
            "  Downloading telethon-1.42.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sqlite3 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sqlite3\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gSfUe7pK0ADQ",
        "outputId": "3eafb49f-9173-4748-a3d4-2b6b489eb916"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.47)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.1)\n",
            "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: langchain-text-splitters\n",
            "Successfully installed langchain-text-splitters-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "print(sqlite3.sqlite_version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iVkGySy95ZJ-",
        "outputId": "c9596771-dc34-443b-ebcc-99e74c6b876a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.37.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nSpz0nOO6Fa2",
        "outputId": "0dc4f140-9797-43cb-d2c7-8bfa9ac06942"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/23.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/23.6 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.0/23.6 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.7/23.6 MB\u001b[0m \u001b[31m278.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m286.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m286.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU --no-deps langchain langchain-text-splitters langchain-community\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "21B6bLec6Ymp",
        "outputId": "38adf6ba-c014-487c-a4b7-522f0df00afb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTe-tPOV4Pde",
        "outputId": "261dd330-75c7-470d-a831-ae93980f0515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ –°–æ–∑–¥–∞–Ω—ã —Ç–∞–±–ª–∏—Ü—ã: documents, document_chunks, chunk_embeddings, metadata\n",
            "‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ 101 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
            "‚úÖ –°–æ–∑–¥–∞–Ω–æ 1103 —á–∞–Ω–∫–æ–≤\n",
            "üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n",
            "‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: 1103 –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
            "\n",
            "üîç –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:\n",
            "\n",
            "–ó–∞–ø—Ä–æ—Å: '—Ö–∞–Ω—Ç—ã–π—Å–∫–∏–π —Ñ–æ–ª—å–∫–ª–æ—Ä'\n",
            "  1. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.7315] **–†–µ—á—å –∏–¥–µ—Ç –æ —Ö–∞–Ω—Ç–∞—Ö –∏–∑ –°—É—Ä–≥—É—Ç—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞...\n",
            "  2. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.6927] ü§© –í–æ-–ø–µ—Ä–≤—ã—Ö, –Ω–µ –≤—Å–µ–º –∑–Ω–∞–∫–æ–º—ã–π —Ö–∞–Ω—Ç—ã–π—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: —Ä–∏—Ç—É–∞–ª—ã –∏ –º–µ–¥–≤–µ–∂—å–∏ –ø–ª—è—Å–∫–∏, –ø—Ä–∞–∫—Ç–∏–∫–∞ –∏–∑–±–µ–≥–∞–Ω–∏—è —Å –∑–∞–∫—Ä—ã—Ç–∏–µ–º –ª–∏—Ü–∞ –ø–ª–∞—Ç–∫–æ...\n",
            "\n",
            "–ó–∞–ø—Ä–æ—Å: '—Ö–∞–Ω—Ç—ã–π—Å–∫–∏–π —è–∑—ã–∫'\n",
            "  1. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.8138] **–†–µ—á—å –∏–¥–µ—Ç –æ —Ö–∞–Ω—Ç–∞—Ö –∏–∑ –°—É—Ä–≥—É—Ç—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞...\n",
            "  2. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.8113] **__–ò–∑ –∫–Ω–∏–≥–∏ –ú–∞–∏–Ω—ã –ê—Ñ–∞–Ω–∞—Å—å–µ–≤–Ω—ã –õ–∞–ø–∏–Ω–æ–π, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∏—Ü—ã –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω–∏—Ü—ã —Å–µ–≤–µ—Ä–Ω–æ–π –≥—Ä—É–ø–ø—ã —Ö–∞–Ω—Ç–æ–≤. –≠—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –∫–∞...\n",
            "\n",
            "–ó–∞–ø—Ä–æ—Å: '—Ç—Ä–∞–¥–∏—Ü–∏–∏ —Ö–∞–Ω—Ç—ã'\n",
            "  1. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.7371] **–†–µ—á—å –∏–¥–µ—Ç –æ —Ö–∞–Ω—Ç–∞—Ö –∏–∑ –°—É—Ä–≥—É—Ç—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞...\n",
            "  2. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.7009] **–•–∞–Ω—Ç—ã–π—Å–∫–∞—è —à—É–±–∞ (–∑–∏–º–∞ –±–ª–∏–∑–∫–æ)**\n",
            "\n",
            "–í—ã—à–µ —è —Å–¥–µ–ª–∞–ª–∞ —Ä–µ–ø–æ—Å—Ç —Å —Ä–∞—Å—Å–∫–∞–∑–æ–º –æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–º —Å–∞–∫–µ ‚Äì —ç—Ç–æ –≤–µ—Ä—Ö–Ω—è—è –∂–µ–Ω—Å–∫–∞—è –º–µ—Ö–æ–≤–∞—è –æ...\n",
            "\n",
            "–ó–∞–ø—Ä–æ—Å: '—è–∑—ã–∫ —Ö–∞–Ω—Ç—ã –ø—Ä–∏–º–µ—Ä—ã'\n",
            "  1. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.7725] **__–ò–∑ –∫–Ω–∏–≥–∏ –ú–∞–∏–Ω—ã –ê—Ñ–∞–Ω–∞—Å—å–µ–≤–Ω—ã –õ–∞–ø–∏–Ω–æ–π, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∏—Ü—ã –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω–∏—Ü—ã —Å–µ–≤–µ—Ä–Ω–æ–π –≥—Ä—É–ø–ø—ã —Ö–∞–Ω—Ç–æ–≤. –≠—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –∫–∞...\n",
            "  2. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.7549] **–†–µ—á—å –∏–¥–µ—Ç –æ —Ö–∞–Ω—Ç–∞—Ö –∏–∑ –°—É—Ä–≥—É—Ç—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞...\n",
            "\n",
            "–ó–∞–ø—Ä–æ—Å: '–∏—Å—á–µ–∑–∞—é—â–∏–µ —è–∑—ã–∫–∏ –°–µ–≤–µ—Ä–∞'\n",
            "  1. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.5871] **__–ò–∑ –∫–Ω–∏–≥–∏ –ú–∞–∏–Ω—ã –ê—Ñ–∞–Ω–∞—Å—å–µ–≤–Ω—ã –õ–∞–ø–∏–Ω–æ–π, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∏—Ü—ã –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω–∏—Ü—ã —Å–µ–≤–µ—Ä–Ω–æ–π –≥—Ä—É–ø–ø—ã —Ö–∞–Ω—Ç–æ–≤. –≠—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –∫–∞...\n",
            "  2. [–°—Ö–æ–¥—Å—Ç–≤–æ: 0.5866] –í–µ—Å—Ç–∏ —Å –ø–æ–ª–µ–π, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–µ–Ω—è –Ω–µ—Çüò≠ –ó–æ–≤—É—Ç —Å–æ–±–∏—Ä–∞—Ç—å –∫–ª—é–∫–≤—É, –≤ –Ω–∞—á–∞–ª–µ –≥–æ–≤–æ—Ä—è—Ç –Ω–∞ —Ö–∞–Ω—Ç—ã–π—Å–∫–æ–º....\n",
            "\n",
            "‚úÖ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ë–∞–∑–∞: khanty_rag.db\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import pickle\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è multilingual –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ===\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self, db_path='rag_khanty.db'):\n",
        "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\"\"\"\n",
        "        self.db_path = db_path\n",
        "        self.conn = sqlite3.connect(db_path)\n",
        "        self.cursor = self.conn.cursor()\n",
        "        self.index = None\n",
        "        self.chunk_ids = []\n",
        "\n",
        "    def create_tables(self):\n",
        "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ç–∞–±–ª–∏—Ü\"\"\"\n",
        "        # –¢–∞–±–ª–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "        self.cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS documents (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                title TEXT NOT NULL,\n",
        "                content TEXT NOT NULL,\n",
        "                doc_type TEXT,\n",
        "                created_at TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # –¢–∞–±–ª–∏—Ü–∞ —á–∞–Ω–∫–æ–≤\n",
        "        self.cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS document_chunks (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                document_id INTEGER NOT NULL,\n",
        "                chunk_text TEXT NOT NULL,\n",
        "                chunk_index INTEGER NOT NULL,\n",
        "                FOREIGN KEY (document_id) REFERENCES documents(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # –¢–∞–±–ª–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ pickle-–±–∞–π—Ç–∞—Ö)\n",
        "        self.cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS chunk_embeddings (\n",
        "                chunk_id INTEGER PRIMARY KEY,\n",
        "                embedding BLOB,\n",
        "                FOREIGN KEY (chunk_id) REFERENCES document_chunks(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "        self.cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS metadata (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                chunk_id INTEGER NOT NULL,\n",
        "                key TEXT NOT NULL,\n",
        "                value TEXT NOT NULL,\n",
        "                FOREIGN KEY (chunk_id) REFERENCES document_chunks(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.commit()\n",
        "        print(\"‚úÖ –°–æ–∑–¥–∞–Ω—ã —Ç–∞–±–ª–∏—Ü—ã: documents, document_chunks, chunk_embeddings, metadata\")\n",
        "\n",
        "    def add_documents(self, documents_data):\n",
        "        \"\"\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: [(title, content, doc_type), ...]\"\"\"\n",
        "        for title, content, doc_type in documents_data:\n",
        "            self.cursor.execute(\n",
        "                \"INSERT INTO documents (title, content, doc_type, created_at) VALUES (?, ?, ?, ?)\",\n",
        "                (title, content, doc_type, datetime.now().isoformat())\n",
        "            )\n",
        "        self.conn.commit()\n",
        "        print(f\"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
        "\n",
        "    def chunk_documents(self, chunk_size=200, chunk_overlap=50):\n",
        "        \"\"\"–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏\"\"\"\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        self.cursor.execute(\"SELECT id, content FROM documents\")\n",
        "        documents = self.cursor.fetchall()\n",
        "\n",
        "        total_chunks = 0\n",
        "        for doc_id, content in documents:\n",
        "            chunks = text_splitter.split_text(content)\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                self.cursor.execute(\n",
        "                    \"INSERT INTO document_chunks (document_id, chunk_text, chunk_index) VALUES (?, ?, ?)\",\n",
        "                    (doc_id, chunk, i)\n",
        "                )\n",
        "                total_chunks += 1\n",
        "\n",
        "        self.conn.commit()\n",
        "        print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–æ {total_chunks} —á–∞–Ω–∫–æ–≤\")\n",
        "\n",
        "    def generate_embeddings(self):\n",
        "        \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ FAISS-–∏–Ω–¥–µ–∫—Å–∞\"\"\"\n",
        "        self.cursor.execute(\"SELECT id, chunk_text FROM document_chunks\")\n",
        "        chunks = self.cursor.fetchall()\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"‚ö†Ô∏è –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\")\n",
        "            return\n",
        "\n",
        "        chunk_texts = [chunk[1] for chunk in chunks]\n",
        "        self.chunk_ids = [chunk[0] for chunk in chunks]\n",
        "\n",
        "        print(\"üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\")\n",
        "        embeddings = model.encode(chunk_texts, convert_to_numpy=True)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î\n",
        "        for chunk_id, embedding in zip(self.chunk_ids, embeddings):\n",
        "            embedding_bytes = pickle.dumps(embedding)\n",
        "            self.cursor.execute(\n",
        "                \"INSERT OR REPLACE INTO chunk_embeddings (chunk_id, embedding) VALUES (?, ?)\",\n",
        "                (chunk_id, embedding_bytes)\n",
        "            )\n",
        "\n",
        "        self.conn.commit()\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ ‚Üí IndexFlatIP + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "\n",
        "        print(f\"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤\")\n",
        "\n",
        "    def search(self, query, k=3):\n",
        "        \"\"\"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É\"\"\"\n",
        "        if self.index is None:\n",
        "            print(\"‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–∑–¥–∞–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ generate_embeddings().\")\n",
        "            return []\n",
        "\n",
        "        query_vec = model.encode([query], convert_to_numpy=True)\n",
        "        faiss.normalize_L2(query_vec)\n",
        "        distances, indices = self.index.search(query_vec.astype('float32'), k)\n",
        "\n",
        "        results = []\n",
        "        for distance, idx in zip(distances[0], indices[0]):\n",
        "            if idx >= len(self.chunk_ids):\n",
        "                continue\n",
        "            chunk_id = self.chunk_ids[idx]\n",
        "\n",
        "            self.cursor.execute('''\n",
        "                SELECT dc.chunk_text, d.title, d.doc_type\n",
        "                FROM document_chunks dc\n",
        "                JOIN documents d ON dc.document_id = d.id\n",
        "                WHERE dc.id = ?\n",
        "            ''', (chunk_id,))\n",
        "\n",
        "            row = self.cursor.fetchone()\n",
        "            if row:\n",
        "                chunk_text, title, doc_type = row\n",
        "                results.append({\n",
        "                    'chunk_id': chunk_id,\n",
        "                    'score': float(distance),\n",
        "                    'text': chunk_text,\n",
        "                    'document_title': title,\n",
        "                    'document_type': doc_type\n",
        "                })\n",
        "        return results\n",
        "\n",
        "    def close(self):\n",
        "        self.conn.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    # === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===\n",
        "    documents_data = []\n",
        "    try:\n",
        "        with open(\"khantystudies_sample.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                post = json.loads(line)\n",
        "                title = f\"–ü–æ—Å—Ç {post['id']}\"\n",
        "                content = post[\"text\"]\n",
        "                doc_type = \"telegram_post\"\n",
        "                documents_data.append((title, content, doc_type))\n",
        "    except FileNotFoundError:\n",
        "        print(\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω :(\")\n",
        "        return\n",
        "\n",
        "    # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã ===\n",
        "    rag_system = RAGSystem(\"khanty_rag.db\")\n",
        "    rag_system.create_tables()\n",
        "    rag_system.add_documents(documents_data)\n",
        "    rag_system.chunk_documents(chunk_size=200, chunk_overlap=50)\n",
        "    rag_system.generate_embeddings()\n",
        "\n",
        "    # === –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–Ω–∞ —Ä—É—Å—Å–∫–æ–º, –ø–æ —Ç–µ–º–µ —Ö–∞–Ω—Ç—ã–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞) ===\n",
        "    test_queries = [\n",
        "        \"—Ö–∞–Ω—Ç—ã–π—Å–∫–∏–π —Ñ–æ–ª—å–∫–ª–æ—Ä\",\n",
        "        \"—Ö–∞–Ω—Ç—ã–π—Å–∫–∏–π —è–∑—ã–∫\",\n",
        "        \"—Ç—Ä–∞–¥–∏—Ü–∏–∏ —Ö–∞–Ω—Ç—ã\",\n",
        "        \"—è–∑—ã–∫ —Ö–∞–Ω—Ç—ã –ø—Ä–∏–º–µ—Ä—ã\",\n",
        "        \"–∏—Å—á–µ–∑–∞—é—â–∏–µ —è–∑—ã–∫–∏ –°–µ–≤–µ—Ä–∞\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nüîç –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:\\n\")\n",
        "    for query in test_queries:\n",
        "        print(f\"–ó–∞–ø—Ä–æ—Å: '{query}'\")\n",
        "        results = rag_system.search(query, k=2)\n",
        "        if results:\n",
        "            for i, res in enumerate(results, 1):\n",
        "                print(f\"  {i}. [–°—Ö–æ–¥—Å—Ç–≤–æ: {res['score']:.4f}] {res['text'][:120]}...\")\n",
        "        else:\n",
        "            print(\"  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
        "        print()\n",
        "\n",
        "    rag_system.close()\n",
        "    print(\"‚úÖ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ë–∞–∑–∞: khanty_rag.db\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}